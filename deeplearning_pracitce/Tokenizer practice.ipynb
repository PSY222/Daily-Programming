{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2290a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'많은': 1, '것을': 2, '바꾸고': 3, '싶다면': 4, '받아들여라': 5}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "paper = ['많은 것을 바꾸고 싶다면 많은 것을 받아들여라.']\n",
    "tknz = Tokenizer()\n",
    "tknz.fit_on_texts(paper)\n",
    "print(tknz.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d4e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('많은', 2), ('것을', 2), ('바꾸고', 1), ('싶다면', 1), ('받아들여라', 1)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a0c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tknz.fit_on_texts(paper)\n",
    "idx_paper = tknz.texts_to_sequences(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f49bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 2, 5]]\n"
     ]
    }
   ],
   "source": [
    "print(idx_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7f8007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "n = len(tknz.word_index) + 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b272989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "idx_onehot = to_categorical(idx_paper,num_classes=n)\n",
    "print(idx_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4530e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01750015  0.04859856 -0.00425328]\n",
      "  [ 0.02181002 -0.00570769  0.04582658]\n",
      "  [-0.03840729 -0.01000639  0.02441927]\n",
      "  [ 0.01821338 -0.01614841  0.04984716]\n",
      "  [ 0.01750015  0.04859856 -0.00425328]\n",
      "  [ 0.02181002 -0.00570769  0.04582658]\n",
      "  [ 0.00289972  0.02693342  0.02434622]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding \n",
    "\n",
    "model =Sequential()\n",
    "model.add(Embedding(input_dim=n,output_dim = 3))\n",
    "model.compile(optimizer='rmsprop',loss='mse')\n",
    "embedding = model.predict(idx_paper)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "n_batch=64\n",
    "epochs=100\n",
    "latent_dim =256\n",
    "n_max_sample = 10000\n",
    "data_path = './data/eng-fra/fra.txt'\n",
    "\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "lines[:10]\n",
    "\n",
    "x_txts=[]\n",
    "y_txts = []\n",
    "x_chars_uni = set()\n",
    "y_chars_uni = set()\n",
    "n_sample = min(n_max_sample,len(lines)-1)\n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "    \n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    \n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)\n",
    "            \n",
    "x_txts[:5]\n",
    "y_txts[:3]\n",
    "\n",
    "x_chars_uni\n",
    "y_chars_uni\n",
    "\n",
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)\n",
    "\n",
    "max_encoder_seq_len = 0\n",
    "\n",
    "for txt in x_txts:\n",
    "     txt_len = len(txt)\n",
    "     max_decoder_seq_len = max(txt_len,max_decoder_seq_len)\n",
    "\n",
    "x_token_idx = {}\n",
    "\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "\n",
    "y_token_idx= {}\n",
    "\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx\n",
    "    \n",
    "encoder_x_data = np.zeros((len(x_txts),max_encoder_seq_len,n_encoder_tokens),dtype='float32')\n",
    "decoder_x_data = np.zeros((len(x_txts),max_encoder_seq_len,n_decoder_tokens),dtype='float32')\n",
    "decoder_y_data = np.zeros((len(x_txts),max_decoder_seq_len,n_decoder_tokens),dtype='float32')\n",
    "\n",
    "for i, x_txt in enumerate(x_txts):\n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i,t,x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i,t+1:,x_token_idx[' ']] =1\n",
    "\n",
    "for i, y_txt in enumerate(y_txts):\n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i,t,y_token_idx[char]] == 1.\n",
    "        if t>0:\n",
    "            decoder_y_data[i,t-1,y_token_idx[char]] = 1.\n",
    "    decoder_x_data[i,t+1:,y_token_idx[' ']] =1.\n",
    "    decoder_y_data[i,t:,y_token_idx[' ']] = 1\n",
    "    \n",
    "    \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "encoder_inputs = Input(shape=(None,n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim,return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,initial_state = encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens,activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)\n",
    "\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentrophy',metrics=['accuracy'])\n",
    "model.fit([encoder_x_data,decoder_x_data],decoder_y_data,batch_size=n_batch,epochs=epochs,validation_split=0.2)\n",
    "\n",
    "\n",
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder(decoder_inputs,initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs]+decoder_states)\n",
    "\n",
    "\n",
    "reverse_x_char_idx = {}\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "    \n",
    "reverse_y_char_idx = {}\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char\n",
    "    \n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1,1,n_decoder_tokens))\n",
    "    y_seq[0,0,y_token_idx['\\t']] = 1\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([y_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "        \n",
    "        y_seq = np.zeros((1,1,n_decoder_tokens))\n",
    "        y_seq[0,0,sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence\n",
    "    \n",
    "\n",
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx:seq_idx+1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence : ',x_txts[seq_idx])\n",
    "    print('Decoded sentence: ',decoded_sentence)\n",
    "    \n",
    "3\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
