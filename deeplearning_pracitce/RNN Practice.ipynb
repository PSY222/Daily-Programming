{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0957e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size,n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4,batch_size,1)\n",
    "    time = np.linspace(0,1,n_steps)\n",
    "    series = 0.5* np.sin(time-offsets1)*(freq1*10+10)\n",
    "    series += 0.2* np.sin(time-offsets2)*(freq2*20+20)\n",
    "    series += 0.1* (np.random.rand(batch_size,n_steps)-0.5)\n",
    "    return series[...,np.newaxis].astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e7900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000,n_steps+1)\n",
    "x_train, y_train = series[:7000,:n_steps], series[:7000,-1]\n",
    "x_valid, y_valid = series[7000:9000,:n_steps], series[7000:9000,-1]\n",
    "x_test, y_test = series[9000:,:n_steps],series[9000:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6b2441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055072278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = x_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da77b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,1]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33134f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1,input_shape=[None,1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e30504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5562ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f6fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(1,n_steps+10)\n",
    "x_new, y_new = series[:,:n_steps],series[:,n_steps:]\n",
    "x = x_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one= model.predict(x[:,step_ahead:])[:,np.newaxis,:]\n",
    "    x = np.concatenate([x,y_pred_one],axis=1)\n",
    "y_pred = x[:,n_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035faa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000,n_steps+10)\n",
    "x_train, y_train = series[:7000,:n_steps], series[:7000,-10:,0]\n",
    "x_valid, y_valid = series[7000:9000,:n_steps], series[7000:9000,-10:,0]\n",
    "x_test, y_test = series[9000:,:n_steps], series[9000:,-10:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42f4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82867069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77066565, -0.25399882,  1.6139213 ,  1.5800487 , -0.9297331 ,\n",
       "         0.86407363,  0.05557621, -0.8509676 ,  1.1286358 ,  0.12193052]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c614afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    y[:,:,step_ahead-1] = series[:,step_ahead:step_ahead+n_steps,0]\n",
    "y_train = y[:7000]\n",
    "y_valid= y[7000:9000]\n",
    "y_test = y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc22ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb600ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(y_true,y_pred):\n",
    "    return keras.metrics.mean_squared_error(y_true[:,-1],y_pred[:,-1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mse',optimizer=optimizer,metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedacb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self,units,activation='tanh',**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def call(self,inputs,states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs,states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091200d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20),return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20),return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1281399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.LSTM(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c03782af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20),return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20),return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f7663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt',shakespeare_url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62424986",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c15d4ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3138b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f38edc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df97cdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d999d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37492b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_txt]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aae05f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4596b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps +1 \n",
    "dataset = dataset.window(window_length,shift=1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54f8fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "881ac370",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1],windows[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77d5cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x_batch, y_batch: (tf.one_hot(x_batch,depth=max_id),y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e83a133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b71e0320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    513/Unknown - 168s 314ms/step - loss: 2.7067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGRU(\u001b[38;5;241m128\u001b[39m,return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m,max_id],\n\u001b[0;32m      3\u001b[0m                     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m      4\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGRU(\u001b[38;5;241m128\u001b[39m,return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m      5\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTimeDistributed(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(max_id,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m ])\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(dataset,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Desktop\\AC\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences=True,input_shape=[None,max_id],\n",
    "                    dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128,return_sequences=True, dropout= 0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer = 'adam')\n",
    "history = model.fit(dataset,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54ddf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    x = np.array(tokenizer.texts_to_sequences(texts))-1\n",
    "    return tf.one_hot(x,max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "809a6130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = preprocess(['How are you'])\n",
    "y_pred = np.argmax(model(x_new),axis=-1)\n",
    "tokenizer.sequences_to_texts(y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fbc1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text,temperature=1):\n",
    "    x_new = preprocess([text])\n",
    "    y_proba = model(x_new)[0,-1:,:]\n",
    "    rescaled_logits = tf.math.log(y_proba)/temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits,num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7a88d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text,n_chars=50,temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text,temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e078090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text('t',temperature=0.2))\n",
    "print(complete_text('w',temperature=1))\n",
    "print(complete_text('w',temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift = n_steps, drop_remainder = True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = datset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1],windows[:,1:]))\n",
    "dataset = dataset.map(lambda x_batch,y_batch: (tf.one_hot(x_batch,depth=max_id),y_batch))\n",
    "dataset = datset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14447784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stateful RNN\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences=True,stateful=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.2,\n",
    "                    batch_input_shape=[batch_size,None,max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2,recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ee276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "model.fit(dataset,steps_per_epoch=steps_per_epoch,epochs=50,callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58dc7c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test,y_test) = keras.datasets.imdb.load_data()\n",
    "np.array(x_train[0][:10],dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9c92bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word,id_ in word_index.items()}\n",
    "\n",
    "for id_, token in enumerate(('<pad>','<sos>','<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "    \n",
    "' '.join([id_to_word[id_] for id_ in x_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4be7bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasetsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
      "     ---------------------------------------- 4.7/4.7 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (0.15.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (1.19.5)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.11.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.3/52.3 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (3.19.6)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (5.2.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: toml in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: six in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.1/140.1 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.9.24)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (4.3.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\ss263\\desktop\\ac\\envs\\tf24\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     -------------------------------------- 211.7/211.7 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ss263\\appdata\\roaming\\python\\python38\\site-packages (from tqdm->tensorflow_datasets) (0.4.5)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=27c2df4f955269ae72f6b2f1e49e5ce71c3c3de22f20f28b36b4b7d95679e4ad\n",
      "  Stored in directory: c:\\users\\ss263\\appdata\\local\\pip\\cache\\wheels\\54\\aa\\01\\724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: tqdm, promise, googleapis-common-protos, etils, dill, tensorflow-metadata, tensorflow_datasets\n",
      "Successfully installed dill-0.3.6 etils-0.9.0 googleapis-common-protos-1.56.4 promise-2.3 tensorflow-metadata-1.11.0 tensorflow_datasets-4.7.0 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "112a8e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ss263\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff26b6cddb74d0ca9f49311b8cdbe19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53e5b85fa4d45c6b74f73454137cf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ss263\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteE8WHE0\\imdb_reviews-train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ss263\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteE8WHE0\\imdb_reviews-test.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ss263\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteE8WHE0\\imdb_reviews-unsup…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\ss263\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews',as_supervised = True, with_info= True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8aa1555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_batch,y_batch):\n",
    "    x_batch = tf.strings.substr(x_batch,0,300)\n",
    "    x_batch = tf.strings.regex_replace(x_batch,b'<br\\\\s*/?>',b\" \")\n",
    "    x_batch = tf.strings.regex_replace(x_batch,b'[^a-zA-Z]',b\" \")\n",
    "    x_batch = tf.strings.split(x_batch)\n",
    "    return x_batch.to_tensor(default_value=b'<pad>'), y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d8e3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "\n",
    "for x_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in x_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51803ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 224494), (b'the', 61156), (b'a', 38569)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70c2888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =10000\n",
    "truncated_vocabulary = [word for word,count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72561a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary),dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words,word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init,num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b6fbf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   24,    12,    13, 10771]], dtype=int64)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaantastic\".split()]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f58dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(x_batch,y_batch):\n",
    "    return table.lookup(x_batch), y_batch\n",
    "\n",
    "train_set = datasets['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size+num_oov_buckets,embed_size,input_shape=[None]),\n",
    "    keras.layers.GRU(128,return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history= model.fit(train_set,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b19b3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m K \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mbackend\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m      3\u001b[0m mask \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m inputs: K\u001b[38;5;241m.\u001b[39mnot_equal(inputs,\u001b[38;5;241m0\u001b[39m))(inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs,0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128,return_sequences=True)(z,mask=mask)\n",
    "z = keras.layers.GRU(128)(z,mask=mask)\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(z)\n",
    "model= keras.Model(inputs=[inputs],outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d35758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None],dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None],dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[],dtype=np.int32)\n",
    "\n",
    "embeddings= keras.layers.Embedding(vocab_size,embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h,state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer= keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,sampler,output_layer=output_layer)\n",
    "final_outputs, final_state,final_sequence_lengths = decoder(\n",
    "decoder_embeddings, initial_state=encoder_state, sequence_length=sequence_lengths)\n",
    "\n",
    "y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "model = keras.Model(inputs=[encoder_inputs,decoder_inputs,sequence_lengths],outputs=[y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225445b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Bidirectional(keras.layers.GRU(10,return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "\n",
    "decoder = tfa.seq2seq.bean_search_decoder.BeamSearchDecoder(\n",
    "cell = decoder_cell, beam_width = beam_width,output_layer = output_layer)\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(encoder_state, multiplier=beam_width)\n",
    "\n",
    "outputs, _, _ = decoder(embedding_decoder,stat_tokens=stat_tokens, end_token=end_token,initial_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f02b31",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e404f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0eeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6ec45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ffe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9bec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251ef4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca0c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
