'''
Source code
https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction/notebook

this is a practice code from Kaggle
'''



import pandas as pandas
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from sklearn.preprocessing import Imputer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectFromModel

from sklearn.mode_selection import StratifiedKFold
from sklearn..model_selection import cross_val_score

from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression

pd.set_option('display.max_columns',100)

trainset = pd.read_csv('../imput/train.csv')
testset = pd.read_csv('../input/test.csv')

trainset.head()

print("Train dataset(rows,cols:", trainset.shape, "\n Test dataset(rows,cols):",testset.shape)


print("Columns in train and not in test dataset:",set(trainset.columns)-set(testset.columns))

data = []

for feature in trainset.columns:
    if feature =="target":
        use ='target'
    elif feature =='id':
        use ='id'
    else: use = 'input'

    if 'bin' in feature or feature =='target':
        type = 'binary'
    elif 'cat' in feature or feature =='id':
        type = 'categorical'
    elif trainset[feature].dtype == float or isinstance(trainset[feature].dtype,float):
        type = 'real'
    elif trainset[feature].dtype == int:
        type = 'integer'
    
    preserve =True
    if feature == 'id':
        preserve =False

    dtype = trainset[feature].dtype
    category = None

    if 'ind' in feature:
        category = 'individual'
    elif 'reg' in feature:
        category = 'registration'
    elif 'car' in feature:
        category = 'car'
    elif 'calc' in feature:
        category = 'calculated'

    feature_dictionary = {
        'varname' : feature,
        'use': use,
        'type':type,
        'preserve':preserve,
        'dtype': dtype,
        'category':category
    }
    data.append(feature_dictionary)

metadata = pd.DataFrame(data, columns = ['varname','use','type','preserve','dtype','category'])
metadata.set_index('varname',inplace=True)
metadata

metadata[(metadata.type == 'categorical')&(metadata.preserve)].index
pd.DataFrame({'count':metadata.groupby(['category'])['category'].size()}).reset_index()

pd.DataFrame({'count':metadata.groupby(['use','type'])['use'].size()}).reset_index()

plt.figure()
fig, ax = plt.subplots(figsize=(6,6))
x = trainset['target'].value_counts().index.values
y = trainset['target'].value_counts().values

sns.barplot(ax = ax , x= x, y=y)
plt.ylabel("Number of values",fontsize=12)
plt.xlabel("Target value",fontsize=12)

plt.tick_params(axis='both',which='major',labelsize=12)
plt.show();

variable = metadata[(metadata.type == 'real')&(metadata.preserve)].index
trainset[variable].describe()

(pow(trainset['ps_car_12']*10.2)).head(10)

(pow(trainset['ps_car_15'],2)).head(10)

sample = trainset.sample(frac=0.05)
var = ['ps_car_12','ps_car_15','target']
sample = sample[var]
sns.pariplot(sample,hue=target,palette='Set1',diag_kind='kde')
plt.show()

var = metadata[(metadata.type == 'real') & (metadata.preserve)].index
i = 0
t1 = trainset.loc[trainset['target'] != 0]
t0 = trainset.loc[trainset['target'] == 0]

sns.set_style('whitegrid')
plt.figure()
fig, ax = plt.subplots(3,4,figsize(16,12))

for feature in  var:
    i += 1
    plt.subplot(3,4,i)
    sns.kdeplot(t1[feature],bw=0.5,label='target=1')
    sns.kdeplot(t0[feature],bw=0.5,label='target=0')
    plt.ylabel('Density Plot',fontsize=12)
    plt.xlabel(feature,fontsize=12)
    locs, labels = plt.xticks()
    plt.tick_params(axis='both',which='major',labelsize=12)
plt.show();

def corr_heatmap(var):
    correlations = trainset[var].corr()
    cmap = sns.diverging_palette(50,10,as_cmap=True)
    fig, ax = plt.subplots(figsize=(10,10))
    sns.heatmap(correlations,cmap=cmap,vmax=1.0,center = 0, fmt='.2f',
    square=True, linewidths=.5,annot=True,cbar_kws={'shrink':.75})
    plt.show();

var = meatdata[(metadat.type=='real') & (metadata.preserve)].index
corr_heatmap(var)

sample = trainset.sample(frac=0.5)
var = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'target']
sample = sample[var]

sns.pairplot(sample,hue='target',palette='Set1',diag_kind='kde')
plt.show()


v = metadata[(metadata.type == 'binary') & (metadata.preserve)].index
trainset[v].describe()


bin_col = [col for col in trainset.columns if '_bin' in col]
zero_list = []
one_list = []

for col in bin_col:
    zero_list.append((trainset[col]==0).sum()/trainset.shape[0]*100)
    one_list.append((trainset[col]==1).sum()/trainset.shape[0]*100)

plt.figure()
fig, ax = plt.subplots(figsize=(6,6))

p1 = sns.barplot(ax=ax, x=bin_col, y=zero_list,color='blue')
p2 = sns.barplot(ax=ax, x=bin_col,y=one_list,bottom=zero_list,color='red')

plt.ylabel('Percent of zero/one [%]',fontsize =12)
plt.xlabel('Binary features',fontsize=12)

locs, labels = plt.xticks()
plt.setp(labels, rotation=90)
plt.tick_params(axis='both',which='major',labelsize=12)
plt.legend((p1,p2),('Zero','One'))
plt.show();

var = metadata[(metadata.type == 'binary') & (metatdata.preserve)].index
var = [col for col in trainset.columns if '_bin' in col]

i = 0 
t1 = trainset.loc[trainset['target'] != 0]
t0 = trainset.loc[trainset['target'] == 0]

sns.set_style('whitegrid')
plt.figure()
fig, ax = plt.subplots(6,3,figsize=(12,24))

for feature in var:
    i += 1
    plt.subplot(6,3,i)
    sns.kdeplot(t1[feature],bw=0.5,label='target=1')
    sns.kdeplot(t0[feature],bw=0.5,label='target=0')
    plt.ylabel('Density plpt',fontsize=12)
    plt.xlabel(feature, fontsize=12)
    loc, labels = plt.xticks()
    plt.tick_params(axis='both',which='major',labelsize=12)
plt.show();


var = metadata[(metadata.type =='categorical') & (metatdata.preserve)].index

for feature in var:
    fig, ax = plt.subplots(figsize=(6,6))
    cat_perc = trainset[[feature,'target']].groupby([feature],as_index=False).mean()
    cat_perc.sort_values(by='target',ascending=False,inplace=True)
    sns.barplot(ax=ax.x=feature,y='target',data=cat_perc,order=cat_perc[feature])
    plt.ylabel('Percent of target with value1',fontsize=12)
    plt.xlabel(feature,fontsize=12)
    plt.tick_params(axis='both',which='major',labelsize=12)
    plt.show();


var = metadata[(metadata.category =='registration') & (metadata.preserve)].index

sns.set_style('whitegrid')

plt.figure()
fig,ax = plt.subplots(1,3, figsize=(12,4))
i = 0

for feature in var:
    i += 1
    plt.subplot(1,3,i)
    sns.kdeplot(trainset[feature],bw=0.5,label="train")
    sns.kdeplot(testset[feature],bw=0.5,label='test')
    plt.ylabel('Distribution',fontsize=12)
    plt.xlabel(feature,fontsize=12)
    locs, labels= plt.xticks()
    plt.tick_params(axis='both',which='major',labelsize=12)
plt.show()


vars_with_missing = []

for feature in trainset.columns:
    missings = trainset[trainset[feature] == -1][feature].count()
    if missings > 0:
        vars_with_missing.append(feature)
        missings_perc = missings/trainset.shape(0)


col_to_drop = trainset.columns[trainset.columns.str.startswith('ps_calc_')]
trainset = trainset.drop(col_to_drop,axis=1)
testset = testset.drop(col_to_drop,axis=1)


def add_noise(series,noise_level):
    return series*(1+noise_level*np.random.randn(len(series)))

def target_encode(trn_series=None,
                    tst_series=None,
                    target = None,
                    min_samples_leaf = 1,
                    smoothing =1,
                    noise_level=0):

    assert len(trn_series) == len(target)
    assert trn_series.name == tst_series.name

    temp = pd.concat([trn_series,target],axis=1)

    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean','count'])

    smoothing = 1/(1+np.exp(-(averages['count']-min_samples_leaf)/smoothing))
    prior = target.mean()

    averages[target.name] = prior * (1-smoothing) + averages['mean']*smoothing

    averages.drop(['mean','count'],axis=1,inplace=True)

    ft_trn_series = pd.merge(trn_series.to_frame(trn_series.name),
                            averages.reset_index().rename(columns={'index':target.name,target.name:'average'}),
                            on = trn_series.name, how='left')['average'].rename(trn_series.name +'_mean').fillna(prior)
    
    ft_trn_series.index = trn_series.index
    ft_tst_series = pd.merge(
        tst_series.to_frame(tst_series.name),
        averages.reset_index().rename(columns={'index':target.name, target.name:'average'}),
        on = tst_series.name, how='left)['average'].rename(trn_series.name + '_mean').fillna(prior)

        ft_tst_series.index = tst_series.index
return add_noise(ft_trn_series,noise_level), add_noise(ft_tst_series,noise_level)

train_encoded, test_encoded = target_encode(trainset['ps_car_11_cat'],
                                testset['ps_car_11_cat'],
                                target = trainset.target,
                                min_samples_leaf=100,
                                smoothing=10,
                                noise_level =0.01)

trainset['ps_car_11_cat_te'] = train_encoded
trainset.drop('ps_car_11_cat',axis=1,inplace=True)
metadata.loc['ps_car_11_cat','keep'] = False
testset['ps_car_11_cat_te'] = test_encoded
testset.drop('ps_car_11_cat',axis=1,inplace=True)

desired_apriori = 0.10

idx_0 = trainset[trainset.target ==0].index
idx_1 = trainset[trainset.target == 1].index

nb_0 = len(trainset.loc[idx_0])
nb_1 = len(trainset.loc[idx_1])

undersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)
undersampled_nb_0 = int(undersampling_rate*nb_0)

undersampled_idx = shuffle(idx_0, random_state=314,n_samples=undersampled_nb_0)
idx_list = list(undersampled_idx) + list(idx_1)

trainset = trainset.loc[idx_list].reset_index(drop=True)

trainset = trainset.replace(-1,np.nan)
testset = testset.replace(-1,np.nan)

for column in cat_features:
    temp = pd.get_dummies(pd.Series(trainset[column]))
    trainset = pd.concat([trainset,temp],axis=1)
    trainset = trainset.drop([column],axis=1)

for column in cat_features:
    temp = pd.get_dummies(pd.Series(testset[column]))
    testset = pd.concat([testset,temp],axis=1)
    testset = testset.drop([column],axis=1)

id_test = testset['id'].values
target_train = trainset['target'].values

trainset = trainset.drop(['target','id'],axis=1)
testset = testset.drop(['id'],axis=1)

class Ensemble(object):
    def __init__(self,n_splits,stacker,base_models):
        self.n_splits = n_splits
        self.stacker =stacker
        self.base_models = base_models
    
    def fit_predict(self,X, y, T):
        X = np.array(X)
        y = np.array(y)
        T = np.array(T)

        folds = list(StratifiedKFold(n_splits=self.n_splits,shuffle=True,random_state=314).split(X,y))

        S_train = np.zeros((X.shape[0],len(self.base_models)))
        S_test = np.zeros((T.shape[0],len(self.base_models)))

        for i , clf in enumerate(self.base_models):
            S_test_i = np.zeros((T.shape[0],self.n_splits))

            for j, (train_idx,test_idx) in enumerate(folds):
                X_train = X[train_idx]
                y_train = y[train_idx]
                X_holdout = X[test_idx]

                print("Base model %d: fit %s model | fold %d" %(i+1,str(clf).split('(')[0],j+1))

                clf.fit(X_train,y_train)
                cross_score = cross_val_score(clf,X_train,y_train,cv=3,scoring='roc_auc')

                print("cross_score [roc-auc]: %.5f [gini] : %.5f"%(cross_score.mean(),2*cross_score.mean()-1))

                y_pred = clf.predict_proba(X_holdout)[:,1]

                S_train[test_idx,i] = y_pred
                S_test_i[:,j] = clf.predict_proba(T)[:,1]
            S_test[:,i] = S_test_i.mean(axis=1)

            results = cross_val_score(self.stacker,S_train,y,cv=3,scoring='roc_auc')

            print("Stacker score[gini] : %.5f"%(2*results.mean()-1))

            self.stacker.fit(S_train,y)
            res = self.stacker.predict_proba(S_test)[:,1]
            return res


       