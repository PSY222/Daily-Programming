{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source:https://www.kaggle.com/code/rohandeysarkar/ultimate-image-classification-guide-2020/notebook\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import MobileNet, MobilieNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Dense, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from kereas.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "path =  '../input/images/dataset/'\n",
    "os.listdir(path)\n",
    "\n",
    "train_df = pd.read_csv(path+'train.csv')\n",
    "test_df = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "#image viewing\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    img = mpimg.imread(path+'/Train Images/' + train_df['image'][i])\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.title(train_df['Class'][i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "train_df['Class'].unique()\n",
    "\n",
    "class_map = {\n",
    "    'Food': 0,\n",
    "    'Attire': 1,\n",
    "    'Decorationandsignage': 2,\n",
    "    'misc': 3\n",
    "}\n",
    "\n",
    "inverse_class_map = {\n",
    "    0: 'Food',\n",
    "    1: 'Attire',\n",
    "    2: 'Decorationandsignage',\n",
    "    3: 'misc'\n",
    "}\n",
    "\n",
    "sns.countplot(train_df['Class'])\n",
    "train_df['Class'].value_counts()\n",
    "\n",
    "balance_attire = (2278 - 1691)\n",
    "balance_decoration = (2278 - 743) \n",
    "balance_misc = (2278 - 1271) \n",
    "balance_food = 1000\n",
    "\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(train_images,to_categorical(train_labels),test_size=0.3,random_state=42)\n",
    "\n",
    "base_model = MobileNet(\n",
    "input_shape=(h,w,3),\n",
    "weights='imagenet',\n",
    "include_top =False,\n",
    "pooling= 'avg')\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "output_class = 4\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Desne(output_class,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                           patience=2,\n",
    "                                           verbose=1,\n",
    "                                           factor=0.5,\n",
    "                                           min_lr = 0.00001)\n",
    "\n",
    "callbacks = [earlystop,learning_rate_reduction]\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),\n",
    "                    validation_data=(x_test,y_test),\n",
    "                   steps_per_epoch = len(x_train)/batch_size,\n",
    "                    epochs=epochs,callbacks=callbacks)\n",
    "\n",
    "labels = model.predict(test_images)\n",
    "print(labels[:4])\n",
    "\n",
    "label = [np.argmax(i) for i in labels]\n",
    "\n",
    "submission = pd.DataFrame({'Image':test_df.Image,'Class':class_label})\n",
    "submission.head()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    img = mpimg.imread(path+ '/Test Images/' + submission['image'][i])\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.title(submission['Class'][i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7da2ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3015624257.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    from tensorflow as tf\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imread   #deprecated\n",
    "\n",
    "from tensorflow as tf\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category = FutureWarning)\n",
    "\n",
    "train_labels = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\")\n",
    "train_labels.head()\n",
    "test_path = \"../input/human-protein-atlas-image-classification/test/\"\n",
    "submission = pd.read_csv(\"../input/human-protein-atlas-image-classification/sample_submission.csv\")\n",
    "\n",
    "test_names = submission.Id.values\n",
    "\n",
    "print(len(test_names))\n",
    "print(test_names[0])\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def fil_tragets(row):\n",
    "    row.Target = np.array(row.Target.split(' ')).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name]= 1\n",
    "    return row\n",
    "\n",
    "for key in label_names.keys():\n",
    "    train_labels[label_name[key]] = 0\n",
    "    \n",
    "train_labels = train_labels.apply(fil_targets,axis=1)\n",
    "train_labels.head()\n",
    "\n",
    "\n",
    "test_labels = pd.DataFrame(data=test_name, columns=['Id'])\n",
    "\n",
    "for col in train_labels.columns.values:\n",
    "    if col != 'Id':\n",
    "        test_labels[col] = 0\n",
    "test_labels.head(1)\n",
    "\n",
    "\n",
    "target_counts = train_labels.drop(['Id','Target'],axis=1).sum(axis=0).sort_values(ascending=False)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.barplot(y=target_counts.index.values, x=target_counts.values, order = target_counts.index)\n",
    "\n",
    "train_labels['number_of_targets'] = train_labels.drop(['Id','Target'],axis=1).sum(axis=1)\n",
    "count_perc = np.round(100*train_labels['number_of_targets'].value_counts()/train_labels.shape[0],2)\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=count_perc.index.values, y= count_perc.values,palette='Reds')\n",
    "plt.xlabel('Number of targets per image')\n",
    "plt.ylabel('% of train data')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(Train_labels[train_labels.number_of_targets>1].drop(\n",
    "    ['Id','Target','number_of_targets'],axis=1).corr(),cmap=\"RdYlBu\",vmin=-1,vmax=1)\n",
    "\n",
    "def find_counts(special_target,labels):\n",
    "    counts = labels[labels[special_target] == 1].drop(\n",
    "        ['Id','Target','number_of_targets'], axis=1).sum(axis=0)\n",
    "    counts = counts[counts>0]\n",
    "    counts = counts.sort_values()\n",
    "    return counts\n",
    "\n",
    "from os import listdir\n",
    "files =  listdir(\"../input/human-protein-atlas-image-classification/train\")\n",
    "\n",
    "for n in range(10):\n",
    "    print(files[n])\n",
    "    \n",
    "def load_image(basepath,image_id):\n",
    "    images = np.zeros(shape=(4,512,512))\n",
    "    images[0,:,:] = imread(basepath + image_id + '_green' +'.png')\n",
    "    images[1,:,:] = imread(basepath + image_id + '_red' +'.png')\n",
    "    images[2,:,:] = imread(basepath + image_id + '_blue' +'.png')    \n",
    "    images[3,:,:] = imread(basepath + image_id + '_yellow' +'.png')\n",
    "    return images\n",
    "\n",
    "def make_image_row(image,subax,title):\n",
    "    subax[0].imshow(image[0], cmap='Greens')\n",
    "    subax[1].imshow(image[1], cmap='Reds')\n",
    "    subax[1].set_title('stained microtubules')\n",
    "    subax[2].imshow(image[2],cmap='Blues')\n",
    "    subax[2].set_title('stained nucleus')\n",
    "    subax[3].imshow(image[3],cmap='Oranges')\n",
    "    subax[3].set_title('stained endoplasmatic reticulum')\n",
    "    subax[0].set_title(title)\n",
    "    return subax\n",
    "\n",
    "def make_title(file_id):\n",
    "    file_targets = train+labels.loc[train_labels.Id == file_id,'Target'].values[0]\n",
    "    title = ' - '\n",
    "    for n in file_targets:\n",
    "        title += label_names[n] + '-'\n",
    "    return title\n",
    "\n",
    "imageloader = TargetGroupIterator()\n",
    "imageloader.find_matching_data_entries()\n",
    "iterator = imageloader.get_loader()\n",
    "\n",
    "file_ids, images = next(iterator)\n",
    "\n",
    "fig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\n",
    "if ax.shape ==(4,):\n",
    "    ax = ax.reshape(1,-1)\n",
    "for n in range(len(file_ids)):\n",
    "    make_image_row(images[n],ax[n],make_title(file_ids[n]))\n",
    "    \n",
    "from sklearn.model_selection import RepeatKFold\n",
    "\n",
    "splitter = RepeatedKFold(n_splits,n_repeats=1,random_state=0)\n",
    "\n",
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition['train'] = train_labels.Id.values[test_idx]\n",
    "    partition['validation'] = train_lables.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    print('Train:',train_idx,'Test:',test_idx)\n",
    "    print('Train:',len(train_idx),'Test:',len(test_idx))\n",
    "    \n",
    "partitions[0]['train'][0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ab5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameter:\n",
    "    def __init__(self,basepath,num_classes=28,\n",
    "                image_rows=512, image_cols = 512,\n",
    "                batch_size=200, n_channels=1,row_scale_factor=4,\n",
    "                col_scale_factor=4, shuffle = False, n_epochs=1):\n",
    "        self.basepath= basepath\n",
    "        self.num_classes = num_classes\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.row_scale_factor = row_scale_factor\n",
    "        self.col_scale_factor = col_scale_factor\n",
    "        self.scaled_row_dim = np.int(self.image_rows/self.row_scale_factor)\n",
    "        self.scaled_col_dim = np.int(self.image_cols/self.col_scale_factor)\n",
    "        self.n_epochs= n_epochs\n",
    "        \n",
    "parameter = ModelParameter(train_path)\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self,modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.basepath = self.parameter.basepath\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "        \n",
    "    def preprocess(self,image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self,image):\n",
    "        image = resize(image,(self,scaled_row_dim,self.scaled_col_dim))\n",
    "        return image\n",
    "    \n",
    "    def reshape(self,image):\n",
    "        image = np.reshape(image,(image.shape[0],image.shape[1],self.n_channels))\n",
    "        return image\n",
    "    \n",
    "    def normalize(self,image):\n",
    "        image /= 255\n",
    "        return image\n",
    "    \n",
    "    def load_image(self,image_id):\n",
    "        image = np.zeros(shape=512,512,4)\n",
    "        image[:,:,0] = imread(self.basepath + image_id +'_green' +'.png')\n",
    "        image[:,:,1] = imread(self.basepath + image_id + '_blue' +'.png')\n",
    "        image[:,:,2] = imread(self.basepath + image_id + '_red' +'.png')        \n",
    "        image[:,:,3] = imread(self.basepath + image_id + '_yellow' +'.png')\n",
    "        return image[:,:,0:self.parameter.n_channels]\n",
    "    \n",
    "    \n",
    "example = images[0,0]\n",
    "preprocessed = preprocessor.preprocess(example)\n",
    "print(example.shape)\n",
    "print(preprocessed.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].imshow(example,cmap='Greens')\n",
    "ax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim),cmap='Greens')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,list_IDs,labels,modelparameter, imagepreprocessor):\n",
    "        self.current_epoch = 0\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.paras.num_classes\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes,random_state=self.current_epoch)\n",
    "            self.current_epoch +=1\n",
    "    \n",
    "    def get_targets_per_image(self,identifier):\n",
    "        return self.labels.loc[self.labels.Id == identifier].drop(['Id','Target','number_of_targets'],axis=1).values\n",
    "    \n",
    "    def __data_generation(self,list_IDs_temp):\n",
    "        x = np.empty((self.batch_size,*self.dim,self.n_channels))\n",
    "        y = np.empty((self.batch_size,self.num_classes),dtype=int)\n",
    "        \n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            x[i] = image\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        x, y = self.__data__generation(list_IDs_temp)\n",
    "        return x,y\n",
    "    \n",
    "    \n",
    "    class PredictGenerator:\n",
    "        def __init__(self,predict_Ids, imagepreprocessor, predict_path):\n",
    "            self.preprocessor = imagepreprocessor\n",
    "            self.preprocessor.basepath = predict_path\n",
    "            self.identifiers = predict_Ids\n",
    "            \n",
    "        def predict(self,model):\n",
    "            y = np.empty(shape=(len(self.identifiers),self.preprocessor.parameter.num_classes))\n",
    "            for n in range(len(self.identifiers)):\n",
    "                image = self.preprocessor.load_image(self.identifiers[n])\n",
    "                image = self.preprocessor.preprocess(image)\n",
    "                image = image.reshape((1,*image.shape))\n",
    "                y[n] = model.predict(image)\n",
    "            return y\n",
    "        \n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout,Flatten\n",
    "    from keras.layers import Conv2D, MaxPooling2D\n",
    "    from keras.losses import binary_crossentropy\n",
    "    from keras.optimizers import Adadelta\n",
    "    from keras.initializers import VarianceScaling\n",
    "    \n",
    "    class BaseLineModel:\n",
    "        def __init__(self,modelparameter):\n",
    "            self.params = modelparameter\n",
    "            self.num_classes = self.params.num_classes\n",
    "            self.img_rows = self.params.scaled_row_dim\n",
    "            self.imag_cols = self.params.scaled_col_dim\n",
    "            self.n_channels = self.params.n_channels\n",
    "            self.input_shape = (self.imag_rows,self.img_cols,self,n_channels)\n",
    "            self.my_metrics = ['accuracy']\n",
    "        \n",
    "        def build_model(self):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=self.input_shape,\n",
    "                                 kernel_initializer = VarianceScaling(seed=0)))\n",
    "            self.model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer=VarianceScaling(seed=0)))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(64,activation='relu',kernel_initializer=VarianceScaling(seed=0)))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "            \n",
    "        def compile_model(self):\n",
    "            self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                              optimizer=keras.optimizers.Adadelta(),\n",
    "                              metrics=self.my_metrics)\n",
    "            \n",
    "        def set_generators(self,trian_generator,validation_generator):\n",
    "            self.training_generator = train_generator\n",
    "            self.validation_generator = validation_generator\n",
    "            \n",
    "        def learn(self):\n",
    "            return self.model.fit_generator(generator=self.training_generator,\n",
    "                                           validation_data = self.validtion_generator,\n",
    "                                           epochs = self.params.n_epochs,\n",
    "                                           use_multiprocessing = True,\n",
    "                                           workers=8)\n",
    "        \n",
    "        def score(self):\n",
    "            return self.model.evaluate_generator(generator=self.vlaidation_generator,\n",
    "                                                use_multiprocessing = True,workers=8)\n",
    "        \n",
    "        def predict(self,predict_generator):\n",
    "            y = predict_generator.predict(self.model)\n",
    "            return y\n",
    "        \n",
    "        def save(self,modeloutputpath):\n",
    "            self.model.save(modeloutputpath)\n",
    "            \n",
    "        def load(self,modellinputpath):\n",
    "            self.model = load_model(modelinputpath)\n",
    "            \n",
    "            \n",
    "    training_generator = DataGenerator(partition['train'],labels,parameter,preprocessor)\n",
    "    validation_generator = DataGenerator(partition['validation'],labels,parameter,preprocessor)\n",
    "    predict_generator = PredictGenerator(partition['validation'],preprocessor,train_path)\n",
    "    \n",
    "    test_preprocessor = ImagePreprocessor(parameter)\n",
    "    submission_predict_generator = PredictGenerator(test_names, test_preprocessor,test_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b97d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.4)\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tdqm import tdqm\n",
    "\n",
    "class_names =['mountain','street','glacier','buildings','sea','forest']\n",
    "class_names_label ={class_name:i for i, class_name in enumerate(class_names)}\n",
    "nb_classess =len(class_names)\n",
    "IMAGE_SIZE = (150,150)\n",
    "\n",
    "def load_data():\n",
    "    datasets = ['../input/seg_train/seg_train', '../input/seg_test/seg_test']\n",
    "    output = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print('Loading {}.format(dataset)')\n",
    "        \n",
    "        for folder in os.listdir(dataset):\n",
    "            label = class_names_label[folder]\n",
    "            for file in tqdm(os.listdir(os.path.join(dataset,folder))):\n",
    "                img_path = os.path.join(os.path.join(dataset,folder),file)\n",
    "                \n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image,IMAGE_SIZE)\n",
    "                \n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images,dtype='float32')\n",
    "        labels = np.array(labels,dtype='int32')\n",
    "        output.append((images,labels))\n",
    "    return output\n",
    "\n",
    "(train_images, train_labels), (test_images,test_labels) = laod_data()\n",
    "train_images, train_labels = shuffle(train_images,train_labels,random_state=25) \n",
    "\n",
    "n_train = train_labels.shape[0]\n",
    "n_test = test_labels.shape[0]\n",
    "\n",
    "print('Number of training examples: {}'.format(n_train))\n",
    "print('Number of testing examples: {}'.format(n_test))\n",
    "\n",
    "import pandas pd\n",
    "_, train_counts = np.unique(train_labels,return_counts=True)\n",
    "_, test_counts = np.unique(test_labels,return_counts=True)\n",
    "pd.DataFrame({'train':train_counts,'test':test_counts},index=class_names).plot.bar()\n",
    "plt.show()\n",
    "\n",
    "plt.pie(train_counts,explode=(0,0,0,0,0,0),labels=class_names,autopct='%1.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Proportion of each observed category')\n",
    "plt.show()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "def display_random_image(class_names,images,labels):\n",
    "    index = np.random.randint(images.shape[0])\n",
    "    plt.figure()\n",
    "    plt.imshow(images[index])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.title('Image #{} :'.format(index) + class_names[labels[index]])\n",
    "    plt.show()\n",
    "    \n",
    "display_random_image(class_names,train_images,train_labels)\n",
    "\n",
    "def display_examples(class_names,images,labels):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.suptitle('Some examples of images of the datset',fontsize=16)\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i],cmap = plt.cm.binary)\n",
    "        plt.xlabel(class_names[labels[i]])\n",
    "    plt.show()\n",
    "    \n",
    "display_examples(class_names,trian_images,train_labels)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(6,activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images,train_labels,batch_size=128,epohs=20,validation_split=0.2)\n",
    "\n",
    "def plot_accuracy_loss(history):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['acc'],'bo--',label='acc')\n",
    "    plt.plot(history.history['val_acc'],'ro--',label='val_acc')\n",
    "    plt.title('train_acc vs val_acc')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['val_loss'],'ro--',label='val_loss')\n",
    "    plt.title('train_loss vs val_loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "\n",
    "test_loss = model.evaluate(test_images,test_labels)\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "pred_labels = np.argmax(predictions,axis=1)\n",
    "\n",
    "display_random_image(class_names,test_images,pred_labels)\n",
    "\n",
    "#what kind of image led the classifier to error?\n",
    "\n",
    "def print_mislabeled_images(class_names,test_images,test_labels,pred_labels_:\n",
    "                            boo = (test_labels == pred_labels)\n",
    "                            mislabeled_indices = np.where(boo == 0)\n",
    "                            mislabeled_images = test_images[mislabeled_indices]\n",
    "                            mislabeled_labels = pred_labels[mislabeled_indices]\n",
    "                            \n",
    "                            title = 'Some examples of mislabled images by the classifier'\n",
    "                            display_examples(class_names, mislabeled_images,mislabeled_labels)\n",
    "\n",
    "print_mislabeled_image(class_names,test_images,test_labels,pred_labels)\n",
    "                            \n",
    "cm = confusion_matrix(test_labels,pred_labels)\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm,annot=True,annot_kws={'size':10},xticklabels=class_names.\n",
    "           y_ticklabels=class_names, ax= ax)\n",
    "                            \n",
    "ax.set_title('Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "model = VGG16(weights='imagenet',include_top=False)\n",
    "                            \n",
    "train_features= model.predict(train_images)\n",
    "test_features= model.predict(test_images)\n",
    "                            \n",
    "n_train,x,y,z = train_features.shape\n",
    "n_train,x,y,z = train_features.shape\n",
    "numFeatures= x*y*z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "x= train_features.reshape((n_train,x*y*z))\n",
    "pca.fit(x)\n",
    "\n",
    "c = pca.transform(x)\n",
    "c1 = c[:,0]\n",
    "c2 =c[:,1]\n",
    "\n",
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(c1[train_labels==i][:1000],c2[train_labels==i][:1000],labels = class_name,alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.title('PCA Projection')\n",
    "    plt.show()\n",
    "\n",
    "                            \n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Faltten(input_shape=(x,y,z)),\n",
    "    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(6,activation=tf.nn.softmax)])\n",
    "\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "history2 = model2.fit(train_features,train_labels,batch_size=128,epochs=15,validation_split=0.2)\n",
    "                            \n",
    "plot_accuracy_loss(history)\n",
    "test_loss = model2.evaluate(test_features,test_labels) \n",
    "\n",
    "#ensemble neural networks\n",
    "\n",
    "np.random.seed(seed=1997)\n",
    "n_estimators = 10\n",
    "max_samples = 0.8\n",
    "\n",
    "max_samples *= n_train\n",
    "max_samples = int(max_samples)\n",
    "\n",
    "models = list()\n",
    "random = np.random.randint(50,100,size=n_estimators)\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(x,y,z)),\n",
    "                                tf.keras.layers.Dense(random[i],activation=tf.nn.relu),\n",
    "                                tf.keras.layers.Dense(6,activation=tf.nn.softmax)])\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    models.append(model)\n",
    "    \n",
    "histories = []\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    train_idx = np.random.choice(len(train_features),size=max_samples)\n",
    "    histories.append(models[i].fit(trian_features[train_idx],train_labels[train_idx],batch_size=128,epochs=10,validation_split=0.1))\n",
    "    \n",
    "predictions = []\n",
    "for i in range(n_estimators):\n",
    "    predictions.append(models[i].predict[test_features])\n",
    "    \n",
    "predictions =np.array(predictions)\n",
    "predictions = predictions.sum(axis=0)\n",
    "pred_labels = predictions.argmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy:{}'.format(accuracy_score(test_labels,pred_labels)))\n",
    "\n",
    "from keras.models import Model\n",
    "model = VGG16(weights='imagenet',include_top=False)\n",
    "model = Model(inputs=model.inputs,outputs=model.layers[-5].output)\n",
    "\n",
    "train_features = model.predict(train_images)\n",
    "test_features = model.predict(test_images)\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Activation,MaxPooling2D,Flatten\n",
    "\n",
    "model2 = VGG16(weights = 'imagenet',include_top = False)\n",
    "input_shape = model2.layers[-4].get_input_shape_at(0)\n",
    "layer_input = Input(shape=(9,9,512))\n",
    "\n",
    "x = layer_input\n",
    "for layer in model2.layers[-4::1]:\n",
    "    x = layer(x)\n",
    "    \n",
    "x = Conv2D(64,(3,3),activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100,activation='relu')(x)\n",
    "x = Dense(6,activation='softmax')(x)\n",
    "\n",
    "new_model = Model(layer_input,x)\n",
    "\n",
    "new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "new_mode.summary()\n",
    "\n",
    "history = new_model.fit(train_features,train_labels,batch_size=128,epochs=10,validation_split=0.2)\n",
    "\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = new_model.predict(test_features)\n",
    "pred_labels = np.argmax(predictions,axis=1)\n",
    "print('Accuracy: { }'.format(accuracy_score(test_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resenet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score,fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from skearn.model_selection import train_test_split\n",
    "\n",
    "workers = 2\n",
    "channel =2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n",
    "\n",
    "fig = plt.figure(figsize=(25,16))\n",
    "\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx,row) in enumerate(df_train.loc[df_train['diagnosis']==class_id].sample(5,random_state=SEED),iterrows()):\n",
    "        ax = fig.add_subplot(5,5,class_id*5+i+1,xticks=[],yticks=[])\n",
    "        path = f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "        image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128) # the trick is to add this line\n",
    "        #previous winner's trick\n",
    "        plt.imshow(image)\n",
    "        ax.set_title(':abel: %d-%d-%s'%(class_id,idx,row['id_code']))\n",
    "        \n",
    "#cropping unnecessary parts\n",
    "\n",
    "def crop_image1(img,tol=7):\n",
    "    mask = img > tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img > tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(0))].shape[0]\n",
    "        if (check_shape==0):\n",
    "            return img\n",
    "        else:\n",
    "            img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae364d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904115d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3acc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
