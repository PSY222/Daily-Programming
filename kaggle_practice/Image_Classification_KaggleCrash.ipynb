{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source:https://www.kaggle.com/code/rohandeysarkar/ultimate-image-classification-guide-2020/notebook\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import MobileNet, MobilieNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Dense, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from kereas.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "path =  '../input/images/dataset/'\n",
    "os.listdir(path)\n",
    "\n",
    "train_df = pd.read_csv(path+'train.csv')\n",
    "test_df = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "#image viewing\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    img = mpimg.imread(path+'/Train Images/' + train_df['image'][i])\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.title(train_df['Class'][i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "train_df['Class'].unique()\n",
    "\n",
    "class_map = {\n",
    "    'Food': 0,\n",
    "    'Attire': 1,\n",
    "    'Decorationandsignage': 2,\n",
    "    'misc': 3\n",
    "}\n",
    "\n",
    "inverse_class_map = {\n",
    "    0: 'Food',\n",
    "    1: 'Attire',\n",
    "    2: 'Decorationandsignage',\n",
    "    3: 'misc'\n",
    "}\n",
    "\n",
    "sns.countplot(train_df['Class'])\n",
    "train_df['Class'].value_counts()\n",
    "\n",
    "balance_attire = (2278 - 1691)\n",
    "balance_decoration = (2278 - 743) \n",
    "balance_misc = (2278 - 1271) \n",
    "balance_food = 1000\n",
    "\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(train_images,to_categorical(train_labels),test_size=0.3,random_state=42)\n",
    "\n",
    "base_model = MobileNet(\n",
    "input_shape=(h,w,3),\n",
    "weights='imagenet',\n",
    "include_top =False,\n",
    "pooling= 'avg')\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "output_class = 4\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Desne(output_class,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                           patience=2,\n",
    "                                           verbose=1,\n",
    "                                           factor=0.5,\n",
    "                                           min_lr = 0.00001)\n",
    "\n",
    "callbacks = [earlystop,learning_rate_reduction]\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),validation_data=(x_test,y_test),\n",
    "                   steps_per_epoch = len(x_train)/batch_size,epochs=epochs,callbacks=callbacks)\n",
    "\n",
    "labels = model.predict(test_images)\n",
    "print(labels[:4])\n",
    "\n",
    "label = [np.argmax(i) for i in labels]\n",
    "\n",
    "submission = pd.DataFrame({'Image':test_df.Image,'Class':class_label})\n",
    "submission.head()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    img = mpimg,imread(path+ '/Test Images/' + submission['image'][i])\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.title(submission['Class'][i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7da2ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3015624257.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    from tensorflow as tf\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imread\n",
    "\n",
    "from tensorflow as tf\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category = FutureWarning)\n",
    "\n",
    "train_labels = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\")\n",
    "train_labels.head()\n",
    "test_path = \"../input/human-protein-atlas-image-classification/test/\"\n",
    "submission = pd.read_csv(\"../input/human-protein-atlas-image-classification/sample_submission.csv\")\n",
    "\n",
    "test_names = submission.Id.values\n",
    "\n",
    "print(len(test_names))\n",
    "print(test_names[0])\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def fil_tragets(row):\n",
    "    row.Target = np.array(row.Target.split(' ')).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name]= 1\n",
    "    return row\n",
    "\n",
    "for key in label_names.keys():\n",
    "    train_labels[label_name[key]] = 0\n",
    "    \n",
    "train_labels = train_labels.apply(fil_targets,axis=1)\n",
    "train_labels.head()\n",
    "\n",
    "\n",
    "test_labels = pd.DataFrame(data=test_name, columns=['Id'])\n",
    "\n",
    "for col in train_labels.columns.values:\n",
    "    if col != 'Id':\n",
    "        test_labels[col] = 0\n",
    "test_labels.head(1)\n",
    "\n",
    "\n",
    "target_counts = train_labels.drop(['Id','Target'],axis=1).sum(axis=0).sort_values(ascending=False)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.barplot(y=target_counts.index.values, x=target_counts.values, order = target_counts.index)\n",
    "\n",
    "train_labels['number_of_targets'] = train_labels.drop(['Id','Target'],axis=1).sum(axis=1)\n",
    "count_perc = np.round(100*train_labels['number_of_targets'].value_counts()/train_labels.shape[0],2)\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=count_perc.index.values, y= count_perc.values,palette='Reds')\n",
    "plt.xlabel('Number of targets per image')\n",
    "plt.ylabel('% of train data')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(Train_labels[train_labels.number_of_targets>1].drop(\n",
    "    ['Id','Target','number_of_targets'],axis=1).corr(),cmap=\"RdYlBu\",vmin=-1,vmax=1)\n",
    "\n",
    "def find_counts(special_target,labels):\n",
    "    counts = labels[labels[special_target] == 1].drop(\n",
    "        ['Id','Target','number_of_targets'], axis=1).sum(axis=0)\n",
    "    counts = counts[counts>0]\n",
    "    counts = counts.sort_values()\n",
    "    return counts\n",
    "\n",
    "from os import listdir\n",
    "files =  listdir(\"../input/human-protein-atlas-image-classification/train\")\n",
    "\n",
    "for n in range(10):\n",
    "    print(files[n])\n",
    "    \n",
    "def load_image(basepath,image_id):\n",
    "    images = np.zeros(shape=(4,512,512))\n",
    "    images[0,:,:] = imread(basepath + image_id + '_green' +'.png')\n",
    "    images[1,:,:] = imread(basepath + image_id + '_red' +'.png')\n",
    "    images[2,:,:] = imread(basepath + image_id + '_blue' +'.png')    \n",
    "    images[3,:,:] = imread(basepath + image_id + '_yellow' +'.png')\n",
    "    return images\n",
    "\n",
    "def make_image_row(image,subax,title):\n",
    "    subax[0].imshow(image[0], cmap='Greens')\n",
    "    subax[1].imshow(image[1], cmap='Reds')\n",
    "    subax[1].set_title('stained microtubules')\n",
    "    subax[2].imshow(image[2],cmap='Blues')\n",
    "    subax[2].set_title('stained nucleus')\n",
    "    subax[3].imshow(image[3],cmap='Oranges')\n",
    "    subax[3].set_title('stained endoplasmatic reticulum')\n",
    "    subax[0].set_title(title)\n",
    "    return subax\n",
    "\n",
    "def make_title(file_id):\n",
    "    file_targets = train+labels.loc[train_labels.Id == file_id,'Target'].values[0]\n",
    "    title = ' - '\n",
    "    for n in file_targets:\n",
    "        title += label_names[n] + '-'\n",
    "    return title\n",
    "\n",
    "imageloader = TargetGroupIterator()\n",
    "imageloader.find_matching_data_entries()\n",
    "iterator = imageloader.get_loader()\n",
    "\n",
    "file_ids, images = next(iterator)\n",
    "\n",
    "fig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\n",
    "if ax.shape ==(4,):\n",
    "    ax = ax.reshape(1,-1)\n",
    "for n in range(len(file_ids)):\n",
    "    make_image_row(images[n],ax[n],make_title(file_ids[n]))\n",
    "    \n",
    "from sklearn.model_selection import RepeatKFold\n",
    "\n",
    "splitter = RepeatedKFold(n_splits,n_repeats=1,random_state=0)\n",
    "\n",
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition['train'] = train_labels.Id.values[test_idx]\n",
    "    partition['validation'] = train_lables.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    print('Train:',train_idx,'Test:',test_idx)\n",
    "    print('Train:',len(train_idx),'Test:',len(test_idx))\n",
    "    \n",
    "partitions[0]['train'][0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ab5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameter:\n",
    "    def __init__(self,basepath,num_classes=28,\n",
    "                image_rows=512, image_cols = 512,\n",
    "                batch_size=200, n_channels=1,row_scale_factor=4,\n",
    "                col_scale_factor=4, shuffle = False, n_epochs=1):\n",
    "        self.basepath= basepath\n",
    "        self.num_classes = num_classes\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.row_scale_factor = row_scale_factor\n",
    "        self.col_scale_factor = col_scale_factor\n",
    "        self.scaled_row_dim = np.int(self.image_rows/self.row_scale_factor)\n",
    "        self.scaled_col_dim = np.int(self.image_cols/self.col_scale_factor)\n",
    "        self.n_epochs= n_epochs\n",
    "        \n",
    "parameter = ModelParameter(train_path)\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self,modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.basepath = self.parameter.basepath\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "        \n",
    "    def preprocess(self,image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self,image):\n",
    "        image = resize(image,(self,scaled_row_dim,self.scaled_col_dim))\n",
    "        return image\n",
    "    \n",
    "    def reshape(self,image):\n",
    "        image = np.reshape(image,(image.shape[0],image.shape[1],self.n_channels))\n",
    "        return image\n",
    "    \n",
    "    def normalize(self,image):\n",
    "        image /= 255\n",
    "        return image\n",
    "    \n",
    "    def load_image(self,image_id):\n",
    "        image = np.zeros(shape=512,512,4)\n",
    "        image[:,:,0] = imread(self.basepath + image_id +'_green' +'.png')\n",
    "        image[:,:,1] = imread(self.basepath + image_id + '_blue' +'.png')\n",
    "        image[:,:,2] = imread(self.basepath + image_id + '_red' +'.png')        \n",
    "        image[:,:,3] = imread(self.basepath + image_id + '_yellow' +'.png')\n",
    "        return image[:,:,0:self.parameter.n_channels]\n",
    "    \n",
    "    \n",
    "example = images[0,0]\n",
    "preprocessed = preprocessor.preprocess(example)\n",
    "print(example.shape)\n",
    "print(preprocessed.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].imshow(example,cmap='Greens')\n",
    "ax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim),cmap='Greens')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,list_IDs,labels,modelparameter, imagepreprocessor):\n",
    "        self.current_epoch = 0\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.paras.num_classes\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes,random_state=self.current_epoch)\n",
    "            self.current_epoch +=1\n",
    "    \n",
    "    def get_targets_per_image(self,identifier):\n",
    "        return self.labels.loc[self.labels.Id == identifier].drop(['Id','Target','number_of_targets'],axis=1).values\n",
    "    \n",
    "    def __data_generation(self,list_IDs_temp):\n",
    "        x = np.empty((self.batch_size,*self.dim,self.n_channels))\n",
    "        y = np.empty((self.batch_size,self.num_classes),dtype=int)\n",
    "        \n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            x[i] = image\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        x, y = self.__data__generation(list_IDs_temp)\n",
    "        return x,y\n",
    "    \n",
    "    \n",
    "    class PredictGenerator:\n",
    "        def __init__(self,predict_Ids, imagepreprocessor, predict_path):\n",
    "            self.preprocessor = imagepreprocessor\n",
    "            self.preprocessor.basepath = predict_path\n",
    "            self.identifiers = predict_Ids\n",
    "            \n",
    "        def predict(self,model):\n",
    "            y = np.empty(shape=(len(self.identifiers),self.preprocessor.parameter.num_classes))\n",
    "            for n in range(len(self.identifiers)):\n",
    "                image = self.preprocessor.load_image(self.identifiers[n])\n",
    "                image = self.preprocessor.preprocess(image)\n",
    "                image = image.reshape((1,*image.shape))\n",
    "                y[n] = model.predict(image)\n",
    "            return y\n",
    "        \n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout,Flatten\n",
    "    from keras.layers import Conv2D, MaxPooling2D\n",
    "    from keras.losses import binary_crossentropy\n",
    "    from keras.optimizers import Adadelta\n",
    "    from keras.initializers import VarianceScaling\n",
    "    \n",
    "    class BaseLineModel:\n",
    "        def __init__(self,modelparameter):\n",
    "            self.params = modelparameter\n",
    "            self.num_classes = self.params.num_classes\n",
    "            self.img_rows = self.params.scaled_row_dim\n",
    "            self.imag_cols = self.params.scaled_col_dim\n",
    "            self.n_channels = self.params.n_channels\n",
    "            self.input_shape = (self.imag_rows,self.img_cols,self,n_channels)\n",
    "            self.my_metrics = ['accuracy']\n",
    "        \n",
    "        def build_model(self):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=self.input_shape,\n",
    "                                 kernel_initializer = VarianceScaling(seed=0)))\n",
    "            self.model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer=VarianceScaling(seed=0)))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(64,activation='relu',kernel_initializer=VarianceScaling(seed=0)))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "            \n",
    "        def compile_model(self):\n",
    "            self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                              optimizer=keras.optimizers.Adadelta(),\n",
    "                              metrics=self.my_metrics)\n",
    "            \n",
    "        def set_generators(self,trian_generator,validation_generator):\n",
    "            self.training_generator = train_generator\n",
    "            self.validation_generator = validation_generator\n",
    "            \n",
    "        def learn(self):\n",
    "            return self.model.fit_generator(generator=self.training_generator,\n",
    "                                           validation_data = self.validtion_generator,\n",
    "                                           epochs = self.params.n_epochs,\n",
    "                                           use_multiprocessing = True,\n",
    "                                           workers=8)\n",
    "        \n",
    "        def score(self):\n",
    "            return self.model.evaluate_generator(generator=self.vlaidation_generator,\n",
    "                                                use_multiprocessing = True,workers=8)\n",
    "        \n",
    "        def predict(self,predict_generator):\n",
    "            y = predict_generator.predict(self.model)\n",
    "            return y\n",
    "        \n",
    "        def save(self,modeloutputpath):\n",
    "            self.model.save(modeloutputpath)\n",
    "            \n",
    "        def load(self,modellinputpath):\n",
    "            self.model = load_model(modelinputpath)\n",
    "            \n",
    "            \n",
    "    training_generator = DataGenerator(partition['train'],labels,parameter,preprocessor)\n",
    "    validation_generator = DataGenerator(partition['validation'],labels,parameter,preprocessor)\n",
    "    predict_generator = PredictGenerator(partition['validation'],preprocessor,train_path)\n",
    "    \n",
    "    test_preprocessor = ImagePreprocessor(parameter)\n",
    "    submission_predict_generator = PredictGenerator(test_names, test_preprocessor,test_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b97d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83b807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae364d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904115d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3acc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
