'''
reference:
https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook

'''

import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder


import one_list
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns

print(os.listdir('../input/application_train.csv'))
print('Training data shape: ', app_train.shape)

app_train.head()

app_test = pd.read_csv('../input/application_test.csv')
print('Testing data shape : ',app_test.shape)
app_test.head()

app_train['TARGET'].value_counts()

app_train['TARGET'].astype(int).plot.hist()


def missing_values_tables(df):
    mis_val = df.isnull().sum()

    mis_val_percent = 100 * df.isnull().sum() / len(df)

    mis_val_table = pd.concat([miss_val,mis_val_percent],axis=1)

    mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 :'Missing Values',1:'% of Total Values'})

    mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values',ascending=False).round(1)

    print('Your selected dataframe has' + str(df.shape[1]) + "columns. \n"
    "There are" + str(mis_val_table_ren_columns.shape[0]) + "columns that have missing values."

    return mis_val_table_ren_columns


missing_values = missing_values_table(app_train)

app_train.dtypes.value_counts()

app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

le = LabelEncoder()
le_count = 0

for col in app_train:
    if app_train[col].dtype == 'object':
        if len(list(app_train[col].unique())) <= 2:
            le.fit(app_train[col])
            app_train[col] = le.transform(app_train[col])
            app_test[col] = le.transform(app_test[col])

            le_count += 1

print("%d columns were label encoded"%le_count)

app_train = pd.get_dummies(app_train)
app_test = pd.get_dummies(app_test)

print('Training Features shape: ', app_train.shape)
print('Testing Features shape: ',app_test.shape)


train_labels = app_train['TARGET']

app_train, app_test = app_train.align(app_test,join='inner',axis =1)

app_train['TARGET'] = train_labels

(app_train['DAYS_BIRTH']/-365).describe()

app_train['DAYS_EMPLOYED'].describe()

app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')
plt.xlabel('Days Employment')

anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]
non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]

app_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243

app_train['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace = True)

app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')
plt.xlabel('Days Employment')

app_test = ['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243
app_test['DAYS_EMPLOYED'].replace({365243:np.nan},inplace = True)

correlations = app_train.corr()['TARGET'].sort_values()


plt.figure(figsize = (10,8))

sns.kdeplot(app_train.loc[app_train['TARGET'] == 0,'DAYS_BIRTH']/365,label = 'target ==0')

sns.kdeplot(app_train.loc[app_train['TARGET'] ==1, 'DAYS_BIRTH']/365,label = 'target==1')

plt.xlabel('Age(years)')
plt.ylabel('Density')
plt.title('Distribution of Ages')


age_data = app_train[['TARGET','DAYS_BIRTH']]
age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365

age_data['YEARS_BINNED'] = pd.cut(age_data['YEAR_BIRTH'])

bins = np.linspace(20,70,num=11)

age_groups = age_data.groupby('YEARS_BINNED').mean()

plt.figure(figsize=(8,8))
plt.bar(age_groups.index.astype(str), 100*groups['TARGET'])

plt.xticks(rotation=75)
plt.xlabel('Age Group (years)')
plt.ylabel('Failure to Repay (%)')
plt.title('Failure to Repay by Age Group')

ext_data = app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]

ext_data_corrs = ext_data.corr()
ext_data_corrs

plt.figure(figsize = (8,6))

sns.heatmap(ext_data_corrs,cmap = plt.cm.RdYlBu_r,vmin=-0.25,annot=True, vmax = 0.6)
plt.title('Correlation Heatmap')

plt.figure(figsize=(10,12))

for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):
    plt.subplot(3,1,i+1)
    sns.kdeplot(app_train.loc[app_train['TARGET']==0,source],label='target==0')
    sns.kdeplot(app_train.loc[app_train['TARGET']==1,source],label='target==1')

    plt.title('Distribution of %s by Target Value'%source)
    plt.xlabel('%s' % source) ; plt.ylabel('Density')

plt.tight_layout(h_pad=2.5)


plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()

plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']

plot_data = plot_data.dropna().loc[:100000,:]

def corr_func(x,y, **kwargs):
    r = np.corrcoef(x,y)[0][1]
    ax = plt.gca()
    ax.annotate('r={:.2f'.format(r),
    xy = (.2,.8),xycoords=ax.transAxes,size=20)

grid = sns.PairGrid(data=plot_data,size=3,diag_sharey =False,hue='TARGET',
            vars = [x for x in list(plot_data.columns) if x != 'TARGET'])

grid.map_upper(plt.scatter, alpha = 0.2)

grid.map_diag(sns.kdeplot)
grid.map_lower(sns.kdeplot,cmap=plt.cm.OrRd_r);

plt.suptitle('Ext Source and Age Features Pairs Plot',size=32,y=1.05)

poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]
poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]

from sklearn.preprocessing imoport Imputer
imputer = Imputer(strategy = 'median')

poly_target = poly_features['TARGET']

poly_features = poly_features.drop(columns= ['TARGET'])

poly_features = imputer.fit_transform(poly_features)
poly_features_test = imputer.fit_transform(poly_features_test)

from sklearn.preprocessing import PolynomialFeatures

poly_transformer = PolynomialFeatures(degree=3)

poly_transformer.fit(poly_features)

poly_features = poly_transformer.transform(poly_features)
poly_features_test = poly_transformer.transform(poly_features_test)
print('Polynomial Features shape: ', poly_features.shape)

poly_transformer.get_feature_names(input_features= ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]

poly_features = pd.DataFrame(poly_features,columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 
                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH'])
poly_corrs = poly_features.corr()['TARGET'].sort_values()

print(poly_corrs.head(10))
print(poly_corrs.tail(5))

poly_features_test = pd.DataFrame(poly_features_test, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 
                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))

poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']
app_train_poly = app_train.merge(poly_features,on='SK_ID_CURR',how='left')

poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']
app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR',how='left')

app_train_poly, app_test_poly = app_train_poly.align(app_test_poly,join='inner',axis=1)

print('Training data with polynomial features shape: ',app_train_poly.shape)
print('Testing data with polynomial features shape: ',app_test_poly.shape)


from sklearn.preprocessing import MinMaxScaler, Imputer
if 'TARGET' in app_train:
    train = app_train.drop(columns=['TARGET'])
else:
    train = app_train.copy()

features = list(train.columns)

test = app_test.copy()

imputer = Imputer(strategy = 'median')

scaler = MinMaxScaler(feature_range=(0,1))
imputer.fit(train)

train = imputer.transform(train)
test = imputer.transform(app_test)

scaler.fit(train)
train = scaler.transform(train)
test = scaler.transform(test)

print('Training data shape: ',train.shape)
print('Testing data shape: ',train.shape)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(C=0.0001)

log_reg.fit(train,train_labels)

log_reg_pred = log_reg.predict_proba(test)[:,1]

submit = app_test[['SK_ID_CURR']]
submit['TARGET'] = log_reg_pred
submit.head()

submit.to_csv('log_reg_baseline.csv',index=False)

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(n_estimators = 100, random_state=50, verbose =1, n_jobs=-1)

random_forest.fit(train,train_labels)
feature_importances = pd.DataFrame({})

feature_importance_values = random_forest.feature_importances
feature_importances = pd.DaataFrame({'feature':features,'importance':feature_importance_values})

predictions = random_forest.predict_proba(test)[:,1]

submit = app_test[['SK_ID_CURR']]
submit['TARGET']= predictions
submit.to_csv('random_forest_baseline.csv',index=False)

poly_feature_names = list(app_train_poly.columns)

imputer = Imputer(strategy = 'median')

poly_features = imputer.fit_tranform(app_train_poly)
poly_features_test = imputer.transform(app_test_poly)

scale = MinMaxScaler(feature_range = (0,1))

poly_features = scaler.fit_transform(poly_features)
poly_features_test = scaler.tranform(poly_features_test)

random_forest_poly = RandomForestClassifier(n_estimators=100,random_state=50, verbose=1,n_jobs=-1)

random_forest_poly.fit(poly_features,train_labels)

predictions = random_forest_poly.predict_proba(poly_features_test)[:,1]
submit = app_test[['SK_ID_CURR']]
submit['TARGET'] = predictions

# Save the submission dataframe
submit.to_csv('random_forest_baseline_engineered.csv', index = False)

def plot_feature_importances(df):
    df = df.sort_values('importance',ascending=False).reset_index()

    df['importanc_normalized'] = df['importance']/df['importance'].sum()

    plt.figure(figsize=(10,6))
    ax = plt.subplot()

    ax.barh(list(reversed(list(df.index[:15]))),df['importance_normalized'].head(15),
        align = 'center',edgecolor = 'k')

    ax.set_yticks(list(reversed(list(df.index[:15]))))
    ax.set_yticklabels(df['feature'].head(15))
    plt.xlabel('Normalized Importance')
    plt.title('Feature Importances')
    plt.show()

    return df

feature_importances_sorted = plot_feature_importances(feature_importances)

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import gca

def model(features,test_features,encoding='ohe',n_folds=5):
    train_ids = features['SK_ID_CURR']
    test_ids = test_features['SK_ID_CURR']

    labels = features['TARGET']

    features = features.drop(columns = ['SK_ID_CURR','TARGET'])
    test_features = test_features.drop(columns=['SK_ID_CURR'])

    if encoding == 'ohe':
        features = pd.get_dummies(features)
        test_features = pd.get_dummies(test_features)

        features, test_features = features.align(test_features,join='inner',axis=1)

        cat_indices ='auto'

    elif encoding =='le':
        label_encoder = LabelEncoder()
        cat_indices = []

        for i , col in enumerate(features):
            if features[col].dtype == 'object':
                features[col] = label_encoder.fit_tranform(np.array(features[col].astype(str)).reshape(-1,)))
                test_features[col] = label_encoder.fit_tranform(np.array(features[col].astype(str)).reshape(-1,)))

                cat_indices.append(i)
    else:
        raise ValueError('Encoding must be either 'ohe' or 'le'')

    print('Training Data Shape: ',features.shape)
    print('Testing Data Shape: ',test_feature.shape)

    feature_names = list(features.columns)

    features = np.array(features)
    test_features = np.array(test_features)

    k_fold = KFold(n_splits = n_folds, shuffle=True, random_state = 50)

    feature_importance_values = np.zeros(len(feature_names))

    test_predictions = np.zeros(features.shape[0])

    valid_scores = []
    train_scores = []

    for train_indices, valid_indices in k_fold.split(features):
        train_features, train_labels = features[train_indices], labels[train_indices]

    model = lgb.LGBMClassifier(n_estimators=10000, objective='binary', class_weight='balanced',learning_rate=0.05, reg_alpha=0.1,reg_lambda=0.1,subsample=0.8,n_jobs=-1.random_state=50)

    model.fit(train_features,train_labels, eval_metric='auc',
                 eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,
                  early_stopping_rounds = 100, verbose = 200)

    best_iteration = model.best_iteration_

    feature_importance_values += model.feature_importances_ / k_fold.n_splits
    test_predictions += model.predict_proba(test_features,num_iteration=best_iteration)[:,1]

     valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']
        
        valid_scores.append(valid_score)
        train_scores.append(train_score)
        
        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()
        
    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})
    
    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})
    
    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)