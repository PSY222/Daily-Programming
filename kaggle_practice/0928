'''
reference:
https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook

'''

import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder


import one_list
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns

print(os.listdir('../input/application_train.csv'))
print('Training data shape: ', app_train.shape)

app_train.head()

app_test = pd.read_csv('../input/application_test.csv')
print('Testing data shape : ',app_test.shape)
app_test.head()

app_train['TARGET'].value_counts()

app_train['TARGET'].astype(int).plot.hist()


def missing_values_tables(df):
    mis_val = df.isnull().sum()

    mis_val_percent = 100 * df.isnull().sum() / len(df)

    mis_val_table = pd.concat([miss_val,mis_val_percent],axis=1)

    mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 :'Missing Values',1:'% of Total Values'})

    mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values',ascending=False).round(1)

    print('Your selected dataframe has' + str(df.shape[1]) + "columns. \n"
    "There are" + str(mis_val_table_ren_columns.shape[0]) + "columns that have missing values."

    return mis_val_table_ren_columns


missing_values = missing_values_table(app_train)

app_train.dtypes.value_counts()

app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

le = LabelEncoder()
le_count = 0

for col in app_train:
    if app_train[col].dtype == 'object':
        if len(list(app_train[col].unique())) <= 2:
            le.fit(app_train[col])
            app_train[col] = le.transform(app_train[col])
            app_test[col] = le.transform(app_test[col])

            le_count += 1

print("%d columns were label encoded"%le_count)

app_train = pd.get_dummies(app_train)
app_test = pd.get_dummies(app_test)

print('Training Features shape: ', app_train.shape)
print('Testing Features shape: ',app_test.shape)


train_labels = app_train['TARGET']

app_train, app_test = app_train.align(app_test,join='inner',axis =1)

app_train['TARGET'] = train_labels

(app_train['DAYS_BIRTH']/-365).describe()

app_train['DAYS_EMPLOYED'].describe()

app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')
plt.xlabel('Days Employment')

anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]
non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]

app_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243

app_train['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace = True)

app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')
plt.xlabel('Days Employment')

app_test = ['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243
app_test['DAYS_EMPLOYED'].replace({365243:np.nan},inplace = True)

correlations = app_train.corr()['TARGET'].sort_values()


plt.figure(figsize = (10,8))

sns.kdeplot(app_train.loc[app_train['TARGET'] == 0,'DAYS_BIRTH']/365,label = 'target ==0')

sns.kdeplot(app_train.loc[app_train['TARGET'] ==1, 'DAYS_BIRTH']/365,label = 'target==1')

plt.xlabel('Age(years)')
plt.ylabel('Density')
plt.title('Distribution of Ages')


age_data = app_train[['TARGET','DAYS_BIRTH']]
age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365

age_data['YEARS_BINNED'] = pd.cut(age_data['YEAR_BIRTH'])

bins = np.linspace(20,70,num=11)

age_groups = age_data.groupby('YEARS_BINNED').mean()

plt.figure(figsize=(8,8))
plt.bar(age_groups.index.astype(str), 100*groups['TARGET'])

plt.xticks(rotation=75)
plt.xlabel('Age Group (years)')
plt.ylabel('Failure to Repay (%)')
plt.title('Failure to Repay by Age Group')

ext_data = app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]

ext_data_corrs = ext_data.corr()
ext_data_corrs

plt.figure(figsize = (8,6))

sns.heatmap(ext_data_corrs,cmap = plt.cm.RdYlBu_r,vmin=-0.25,annot=True, vmax = 0.6)
plt.title('Correlation Heatmap')

